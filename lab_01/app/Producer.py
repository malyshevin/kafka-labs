import time
from confluent_kafka import Producer
from utils import (
    get_env,
    get_logger,
    CustomMessage,
)

logger = get_logger(__name__)


def create_producer(default_servers="localhost:9092"):
    conf = {
        "bootstrap.servers": get_env("KAFKA_BOOTSTRAP_SERVERS", default_servers),
        "acks": "all",               # –ì–∞—Ä–∞–Ω—Ç–∏—è —Ç–æ–≥–æ, —á—Ç–æ –≤—Å–µ —Ä–µ–ø–ª–∏–∫–∏ –ø–æ–¥—Ç–≤–µ—Ä–¥—è—Ç –ø–æ–ª—É—á–µ–Ω–∏–µ
        "retries": 5,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        "enable.idempotence": True,  # –í–∫–ª—é—á–∞–µ–º –∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    }
    return Producer(conf)


def delivery_report(err, msg):
    if err is not None:
        logger.warning(f"üö´ Message delivery failed: {err}")
        return

    logger.info(f"‚úÖ Message delivered to {msg.topic()} [{msg.partition()}]")


def produce_messages(producer, default_topic="default_topic", cnt_messages=100, ms_between_messages=0):
    topic_name = get_env("KAFKA_TOPIC", default_topic)

    for i in range(1, cnt_messages + 1):
        msg = CustomMessage(key=f"user_{i % 10}", value=f"Message {i}")
        serialized_msg = msg.to_json(logger)

        producer.produce(topic=topic_name, key=msg.key, value=serialized_msg, callback=delivery_report)
        producer.poll(0)  # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–ª–ª–±–µ–∫–æ–≤

        time.sleep(ms_between_messages)

    producer.flush()  # –ñ–¥–µ—Ç, –ø–æ–∫–∞ –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –±—É–¥—É—Ç –ø–µ—Ä–µ–¥–∞–Ω—ã –∏–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã


if __name__ == "__main__":
    producer = create_producer()
    produce_messages(producer)
